<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

    <title> &middot; source{d} blog</title>
    <meta name="author" content="source{d}">
    <meta name="description" content="coding to find awesome engineers">
    <meta name="generator" content="Hugo 0.15" />
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">

    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/normalize/2.1.2/normalize.min.css">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="/css/screen.css">
    <link rel="stylesheet" href="/css/github.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/styles/default.min.css">

    <!-- Stylesheet for theme color -->
    <style type="text/css">
    a, a:visited {color: #32526A; font-weight: 600; }
    .pagination a {color: #111111;}
    .gist .gist-file .gist-meta a:visited {color: #111111 !important;}
    a:focus, a:hover {color: #32526A;}
    h1.post-title, h1.blog-title a, h1.blog-title a {
      color: #32526A;
    }

    h1.post-title a:focus, h1.post-title a:hover, h1.blog-title a:focus, h1.blog-title a:hover {
      color: #000;
    }
    .older-posts:hover, .newer-posts:hover {color: #32526A;}

    .post-meta .authorimage {
      zoom:.5;
    }

    .post-meta {
      text-transform: none;
    }

    .post .post-meta {
      margin-bottom: 50px;
    }

    .preview .post-meta p { margin:0; }
    .preview .post-meta .authorimage {
      display:none;
    }

    .preview .image  {
      height: 90px;
      margin: auto;
      overflow: hidden;
      background-size: cover;
      background-position: center center;
    }

    .preview .image img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
    }
</style>

</head>
<body class="post-template">

    <div id="nav">
  <ul class="nav-list">
    <li class="nav-list-item"><a href="http://sourced.tech">sourced.tech</a></li>
  </ul>
</div>


<header id="site-head">
	
	<h1 class="blog-title"><a href="/">source{d}</a></h1>
	
	
	<h1 class="blog-subtitle">coding to find awesome engineers</h1>
	
  <span class="blog-sub-link">Take me to <a href="http://sourced.tech">sourced.tech</a></span>
</header>
    

    <main class="content" role="main">
	    <article class="post">
	        <header>
	        <h1 class="post-title"></h1>
	        <section class="post-content">
	            

<p><img src="http://blog.sourced.tech/post/github_topic_modeling/wordcloud.png" alt="word cloud" />
<a href="https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5">Тематическое моделирование</a> - подраздел машинного обучения, посвященный извлечению абстрактных &ldquo;тем&rdquo; из набора &ldquo;документов&rdquo;. Каждый &ldquo;документ&rdquo; представлен <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">мешком слов</a>, т.е. множеством слов вместе с их частотами. Введение в тематическое моделирование прекрасно описано проф. <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%A3%D1%87%D0%B0%D1%81%D1%82%D0%BD%D0%B8%D0%BA:Vokov">К. В. Воронцовым</a> в лекциях ШАД [<a href="http://www.machinelearning.ru/wiki/images/e/e6/Voron-ML-TopicModeling-slides.pdf">PDF</a>]. Самая известная модель ТМ - это, конечно, <a href="https://ru.wikipedia.org/wiki/%D0%9B%D0%B0%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D0%B5_%D0%94%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5">Латентное размещение Дирихле</a> (LDA). Константину Вячеславовичу удалось обобщить все возможные тематические модели на основе мешка слов в виде <a href="http://link.springer.com/article/10.1007/s10994-014-5476-6">аддитивной регуляризации</a> (ARTM). В частности, LDA тоже входит в множество моделей ARTM. Идеи ARTM воплощены в проекте <a href="https://github.com/bigartm/bigartm">BigARTM</a>.</p>

<p>Обычно тематическое моделирование применяют к текстовым документам. Мы в source{d}
(стартап в Испании) перевариваем биг дату, полученную из GitHub репозиториев (
и скоро примемся за каждый публично доступный репозиторий в мире). Естественным
образом возникла идея интерпретировать каждый репозиторий как мешок слов и натравить BigARTM. В этой статье пойдет речь о том как мы выполнили по сути первое в мире тематическое исследование крупнейшего хранилища open source проектов, что из этого получилось и как это повторить. <strong>docker inside!</strong>
<habracut/></p>

<p>TL;DR:</p>

<pre><code>docker run srcd/github_topics apache/spark
</code></pre>

<p>(заменить <code>apache/spark</code> на любой GitHub реп по желанию).</p>

<p><a href="https://storage.googleapis.com/github-repositories-topic-modeling/topics.ods">Таблица OpenDocument с извлеченными темами.</a></p>

<p><a href="https://storage.googleapis.com/github-repositories-topic-modeling/topics.json">JSON с извлеченными темами.</a></p>

<p><a href="https://storage.googleapis.com/github-repositories-topic-modeling/repo_topic_modeling.pickle.gz">Обученная модель</a> - 40МБ, gzipped pickle для Python 3.4+, Pandas 1.18+.</p>

<p>[Dataset на data.world]() - скоро загрузим.</p>

<h2 id="теория:3d1ad5dc492a464c9ab9cf6a149322ec">Теория</h2>

<p>Задана тематическая вероятностная модель на множестве документов
$$\inline D$$ которая описывает частоту появления слова $$\inline w$$ в документе $$\inline d$$ с темами $$\inline t$$:</p>

<p>$$
p(w|d) = \sum_{t\in T} p(w|t) p(t|d)
$$</p>

<p>где $$\inline p(w|t)$$ - вероятность отношения слова $$\inline w$$ к теме $$\inline t$$,
$$\inline p(t|d)$$ - вероятность отношения темы $$\inline t$$ к документу $$\inline d$$,
т.о. формула выше это просто выражение <a href="https://ru.wikipedia.org/wiki/%D0%A4%D0%BE%D1%80%D0%BC%D1%83%D0%BB%D0%B0_%D0%BF%D0%BE%D0%BB%D0%BD%D0%BE%D0%B9_%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8">полной вероятности</a>,
при условии истинности гипотезы независимости случайных величин: $$\inline p(w|d,t) = p(w|t)$$.
Слова берутся из словаря $$\inline W$$, темы принадлежат множеству $$\inline T$$ которое представляет собой просто серию индексов $$\inline [1, 2, \dots n_t]$$.</p>

<p>Нам нужно восстановить $$\inline p(w|t)$$ и $$\inline p(t|d)$$ из
заданного набора документов $$\inline \left{d\in D: d = \left{w<em>1 \dots w</em>{n<em>d}\right}\right}$$.
Обычно считают что $$\inline \hat{p}(w|d) = \frac{n</em>{dw}}{n<em>d}$$, где $$\inline n</em>{dw}$$ -
количество вхождений $$\inline w$$ в документ $$\inline d$$,
однако это подразумевает что все слова одинаково важные что не всегда справедливо.
Под &ldquo;важностью&rdquo; здесь имеется в виду мера, негативно скоррелированная с общей частотой появления слова в документах.
Обозначим восстанавливаемые вероятности $$\inline \hat{p}(w|t) = \phi<em>{wt}$$ и $$\inline \hat{p}(t|d) = \theta</em>{td}$$.
Т.о. наша задача сводится к стохастическому матричному разложению, которая некорректно поставлена:</p>

<p>$$
\frac{n_{dw}}{n_d} ≈ \Phi \cdot \Theta = (\Phi S)(S^{-1}\Theta) = \Phi&rsquo; \cdot \Theta&rsquo;
$$</p>

<p>В задачах машинного обучения обычно применяют <a href="https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0">регуляризацию</a>) как способ улучшить характеристики модели на неизвестных данных (как следствие, уменьшается <a href="https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">переобучение</a>, сложность и т.д.); в нашем случае, она просто <strong>необходима</strong>.</p>

<p>Задачи наподобие описанной выше решаются с помощью
<a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%BE%D0%BF%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D1%8F">метода максимального правдоподобия</a>:</p>

<p>$$
\sum<em>{d\in D}\sum</em>{w\in d}n<em>{dw}\ln \sum</em>{t}\phi<em>{wt} \theta</em>{td} \to \max_{\Phi,\Theta}
$$</p>

<p>при условиях</p>

<p>$$
\phi<em>{wt} &gt; 0; \sum</em>{w\in W}\phi<em>{wt} = 1;
\theta</em>{td} &gt; 0; \sum<em>{t\in T}\theta</em>{td} = 1.
$$</p>

<p>Суть ARTM в том чтобы естественным образом добавить регуляризацию в виде дополнительных слагаемых:</p>

<p>$$
\sum<em>{d\in D}\sum</em>{w\in d}n<em>{dw}\ln \sum</em>{t}\phi<em>{wt} \theta</em>{td} + R(\Phi,\Theta) \to \max_{\Phi,\Theta}
$$</p>

<p>Поскольку это простое сложение, мы можем комбинировать различные регуляризаторы в одной оптимизации,
например, проредить матрицу и увеличить независимость тем. LDA формулируется в терминах ARTM так:</p>

<p>$$
R(\Phi,\Theta)<em>{Dirichlet} = \sum</em>{t,w} (\beta<em>w - 1)\ln \phi</em>{wt} + \sum_{d,t} (\alpha<em>t - 1)\ln \theta</em>{t,d}
$$</p>

<p>Переменные $$\inline \Phi$$ и $$\inline \Theta$$ могут быть эффективно вычислены с помощью итеративного
<a href="https://ru.wikipedia.org/wiki/EM-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC">EM-алгоритма</a>.
Десятки готовых ARTM регуляризаторов готовы к бою в составе <a href="https://github.com/bigartm/bigartm">BigARTM</a>.</p>

<p>На этом вынужденный рерайт лекции ШАДа заканчивается и начинается</p>

<h2 id="практика:3d1ad5dc492a464c9ab9cf6a149322ec">Практика</h2>

<p>Для анализа на октябрь 2016 года были доступны около 18 миллионов репозиториев на GitHub. Их на самом деле гораздо больше, просто мы отбросили форки и &ldquo;хард форки&rdquo; (форк не отмечен GitHub-ом).  Положим каждый репозиторий это $$d$$, а каждое имя в исходниках это $$w$$.
Анализ исходников делался теми же инструментами, что и при глубоком обучении исходному коду в ранних экспериментах
(см. наши презентации с последних конференций <a href="https://www.re-work.co/">RE·WORK</a>:
<a href="https://goo.gl/4zq8g9">Берлин</a> и <a href="https://goo.gl/wRQCLS">Лондон</a>): первичная классификация
<a href="https://github.com/github/linguist">github/linguist</a>-ом и парсинг на основе <a href="http://pygments.org/">Pygments</a>.
Текстовые файлы общего назначения отбрасывались, например README.md.</p>

<p>Имена из исходников следует извлекать не &ldquo;в лоб&rdquo;, например, <code>class FooBarBaz</code> добавляет 3 слова в мешок: <code>foo</code>, <code>bar</code> и <code>baz</code>, а
<code>int wdSize</code> добавляет два: <code>wdsize</code> и <code>size</code>. Кроме того, имена стеммировались <a href="http://snowballstem.org/">Snowball</a>-ом из <a href="http://www.nltk.org/">NLTK</a>, хотя специально мы не исследовали пользу от этого. Последний этап предобработки заключался в вычислении логарифмической версии
<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> взвешивания
(снова, специально не исследовали, просто копировали
<a href="https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering">решения</a> из обычного НЛП) и фильтрации слишком редких и общеупотребительных имен, в нашем случае, границы были 50 и 100000 соответственно.</p>

<p>После того как ARTM выдал результат, нужно было вручную дать имена темам, основываясь на
ключевых словах и репозиториях-представителях. Число тем было выставлено в 200, и как потом оказалось,
нужно было поставить больше, т.к. тем на Гитхабе очень много. Нудная работа отняла целую неделю.</p>

<p>Предобработка была выполнена на <a href="https://cloud.google.com/dataproc/">Dataproc</a> aka Spark в облаке Google, а основные действия производились локально на мощном компьютере. Результирующая разреженная матрица имела размер около 20 GB, и ее пришлось преобразовать в текстовый формат Vowpal Wabbit, чтобы ее смог переварить BigARTM CLI. Данные были перемолоты довольно быстро, за пару часов:</p>

<pre><code>bigartm -c dataset_vowpal_wabbit.txt -t 200 -p 10 --threads 10 --write-model-readable bigartm.txt --regularizer &quot;0.05 SparsePhi&quot; &quot;0.05 SparseTheta&quot;
Parsing text collection... OK.  
Gathering dictionary from batches... OK.  
Initializing random model from dictionary... OK.  
Number of tokens in the model: 604989
================= Processing started.
Perplexity      = 586350
SparsityPhi     = 0.00214434
SparsityTheta   = 0.422496
================= Iteration 1 took 00:11:57.116
Perplexity      = 107901
SparsityPhi     = 0.00613982
SparsityTheta   = 0.552418
================= Iteration 2 took 00:12:03.001
Perplexity      = 60701.5
SparsityPhi     = 0.102947
SparsityTheta   = 0.768934
================= Iteration 3 took 00:11:55.172
Perplexity      = 20993.5
SparsityPhi     = 0.458439
SparsityTheta   = 0.902972
================= Iteration 4 took 00:11:56.804
...
</code></pre>

<p><code>-p</code> задает число итераций. Уверенности в том, какие регуляризаторы использовать, не было, так что активирована только &ldquo;sparsity&rdquo;. Сказалось отсутствие подробной документации (разработчики пообещали это исправить). Важно отметить, что объем оперативной памяти, который потребовался для работы на пике был не больше 30 GB, что очень круто на фоне <a href="https://radimrehurek.com/gensim/">gensim</a> и, прости господи,
<a href="http://scikit-learn.org/stable/">sklearn</a>.</p>

<h2 id="темы:3d1ad5dc492a464c9ab9cf6a149322ec">Темы</h2>

<p>В итоге, 200 тем можно поделить на следующие группы:</p>

<ul>
<li><strong>Понятия</strong> - общие, гирокие и абстрактные.</li>
<li><strong>Человеческие языки</strong> - оказалось, что по коду можно примерно определить родной язык программиста, наверное, отчасти благодаря смещению от стемминга.</li>
<li><strong>Языки программирования</strong> - не то чтобы интересные, т.к. эту информацию мы и так знаем.
У ЯП обычно есть стандартная библиотека aka &ldquo;батарейки&rdquo; из классов и функций, которые импортируются/включаются в исходники, и соответствующие имена детектируются нашим тематическим моделированием.
Некоторые темы оказались у́же чем ЯП.</li>
<li><strong>Общий IT</strong> - попали бы в <em>Понятия</em> если имели выразительный список ключевых слов. Репозитории часто ассоциируются с уникальным набором имен, например, говорим Рельсы, держим в уме ActiveObject и прочий Active. Частично перекликается с <a href="https://habrahabr.ru/post/247363/">Философия программирования 2 — Миф и язык</a>.</li>
<li><strong>Сообщества</strong> - посвященные конкретным, потенциально узким технологиям или продуктам.</li>
<li><strong>Игры</strong>.</li>
<li><strong>Бред</strong> - 2 темам так и не удалось найти разумное объяснение.</li>
</ul>

<h3 id="понятия:3d1ad5dc492a464c9ab9cf6a149322ec">Понятия</h3>

<p>Пожалуй, самая интересная группа с кучей извлеченных фактов из повседневной жизни:</p>

<ol>
<li>В пицце есть сыр, а еще ее упоминают многие репозитории.</li>
<li>Термины из математики, линейной алгебры, криптографии, машинного обучения, цифровой обработки сигналов, генной инженерии, физики элементарных частиц.</li>
<li>Дни недели. Monday, Tuesday и т.д.</li>
<li>Всевозможные факты и персонажи из RPG и других игр в жанре фэнтези.</li>
<li>В IRC есть псевдонимы.</li>
<li>Множество шаблонов проектирования (говорим за них спасибо Java и PHP).</li>
<li>Цвета. Включая некоторые экзотические (говорим за них спасибо <a href="https://habrahabr.ru/company/ua-hosting/blog/269013/">CSS</a>).</li>
<li>В электронной почте есть CC, BCC, и ее посылают протоколом SMTP и получают POP/IMAP-ом.</li>
<li>Как создать хороший datetime picker. Кажется, это весьма типичный проект на GitHub, хе-хе.</li>
<li>Люди работают за деньги и тратят их на покупку домов и вождение (очевидно, из дома на работу и назад).</li>
<li>Всевозможное &ldquo;железо&rdquo;.</li>
<li>Исчерпывающий список терминов HTTP, SSL, Internet, Bluetooth и WiFi.</li>
<li>Все что вы хотели бы узнать про управление памятью.</li>
<li>Что гуглить есть хочется сделать свою прошивку на основе Android.</li>
<li>Штрихкоды. Огромное число разных видов.</li>
<li>Люди. Они делятся на мужчин и женщин, они живут и занимаются сексом.</li>
<li>Отличный список текстовых редакторов.</li>
<li>Погода. Много типичных слов.</li>
<li>Открытые лицензии. Вообще говоря, они не должны были попасть в отдельную тему т.к. имена и тексты лицензий в теории не пересекаются. По опыта работы с Pygments, некоторые ЯП поддерживаются гораздо хуже чем другие и, судя по всему, некоторые были неправильно распарсены.</li>
<li>Коммерция. У магазины предлагают скидки и продают товары покупателям.</li>
<li>Биткоины и блокчейн.</li>
</ol>

<h3 id="человеческие-языки:3d1ad5dc492a464c9ab9cf6a149322ec">Человеческие языки</h3>

<p>В список тем вошли испанский, португальский, французский и китайский. Русский в отдельную тему не сформировался, что свидетельствует скорее о более высоком уровне наших программистов на GitHub, пишущих сразу на английском, чем о малом количестве русских репозиториев. В этом смысле, китайские репозитории убивают.</p>

<h3 id="языки-программирования:3d1ad5dc492a464c9ab9cf6a149322ec">Языки программирования</h3>

<p>Интересная находка в ЯП - это тема &ldquo;Non-native English PHP&rdquo;, ассоциированная с проектами на PHP, не написанными людьми, знающими английский. Видимо, эти две группы программистов как-то принципиально по-разному пишут код. Кроме того, есть две темы, относящиеся к Java: JNI и байткод.</p>

<h3 id="общий-it:3d1ad5dc492a464c9ab9cf6a149322ec">Общий IT</h3>

<p>Здесь не так интересно. Существует много репозиториев с ядрами ОС - большие, шумные и несмотря на наши старания они изгадили некоторые топики. Тем не менее, кое-что стоит упомянуть:</p>

<ul>
<li>Много информации о дронах. Они работают на Linux.</li>
<li>Есть много реализаций Ruby. Часто встречаются &ldquo;экстремальные форки&rdquo;, когда люди берут чужую кодовую базу и комиттят единым целым с потерей истории изменений.</li>
<li>onmouseup, onmousedown и onmousemove - три гиганта, на которых стоит UI.</li>
<li>Огромное число баззвордов и технологий из мира Javascript.</li>
<li>Платформы для онлайн обучения. Особенно <a href="https://moodle.org/">Moodle</a>. Много, много Moodle.</li>
<li>Все когда-либо созданные открытые CMS.</li>
<li>Тема &ldquo;Coursera Machine Learning&rdquo; предоставляет отличный список репозиториев с домашней работой на курсах Coursera, посвященных машинному обучению.</li>
</ul>

<h3 id="сообщества:3d1ad5dc492a464c9ab9cf6a149322ec">Сообщества</h3>

<p>Самая большая по численности тем группа, почти 100. Много репозиториев оказались частными облачными хранилищами конфигураций для текстовых редакторов, особенно Vim и Emacs. Т.к. у Vim всего одна тема, в то время как у Emacs две, надеюсь, это поставит окончательную точку в споре какой редактор лучше!</p>

<p>Встретились сайты на всех известных веб движках, написанных на Python, Ruby, PHP, Java, Javascript и т.д. Сайты на PHP используют движки Wordpress, Joomla, Yii, VTiger, Drupal, Zend, Cake и Symphony почему-то обязательно с Doctrine (по теме на каждый).
Python: Django, Flask, Google AppEngine. Ruby: Rails и только Rails. <a href="https://habrahabr.ru/post/301532/">Реееельсы</a>. Сайты на Java
сколлапсировали в одну смешанную тему. И конечно же нашлось место сайтам на Node.js.</p>

<p>Оказалось, что много проектов используют <a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a> - открытый движок OCR. Кроме того, многие используют <a href="https://github.com/BVLC/caffe">Caffe</a> (и ни разу не Tensorflow).</p>

<p>Quake 3 / idTech 3 настолько популярен в геймдеве что заслужил отдельную тему. У Unity3D  их две, причем первая в основной массе это сборище студенческих проектов и домашних поделок.
Cocos2D также популярен и получил 2 темы. Наконец, нашлись 3 темы про OpenGL + WebGL.
Наверное, разница в способе работы с API и используемой обвязке (GLUT и т.п.).</p>

<p>Неудивительно, что <a href="https://github.com/chef/chef">Chef</a> - средство управления конфигурациями -
поделил тему с готовкой еды (recipes, kitchen и т.п).
Однако, WinAPI неожиданно оказался в одной теме с репозиториями про покемонов. Есть предположение, что стемминг сделал характерные имена WinAPI похожими на имена покемонов&hellip;</p>

<h3 id="игры:3d1ad5dc492a464c9ab9cf6a149322ec">Игры</h3>

<p>Много тем связаны с <a href="http://www.libsdl.org/">SDL</a>, а также с Minecraft и RPG.</p>

<h2 id="что-можно-скачать:3d1ad5dc492a464c9ab9cf6a149322ec">Что можно скачать</h2>

<p>Мы приготовили образ Docker, чтобы любой желающий мог запустить нашу обученную модель
на произвольном репозитории с GitHub. Нужно просто выполнить</p>

<pre><code>docker run srcd/github_topics apache/spark
</code></pre>

<p>и вы увидите топ 5. Внутри образа зашита сериализованная матрица тем и слов, она доступна отдельно: <a href="https://storage.googleapis.com/github-repositories-topic-modeling/repo_topic_modeling.pickle.gz">link</a>.
Формат - <a href="https://docs.python.org/3/library/pickle.html#pickle-protocols">pickle 4-ой версии</a>
с кортежем длины 2, первый элемент это Pandas 1.8+ <a href="http://pandas.pydata.org/pandas-docs/stable/sparse.html?highlight=sparsedataframe">SparseDataFrame</a>,
и второй - список с IDF. Кроме того, есть
<a href="https://storage.googleapis.com/github-repositories-topic-modeling/topics.ods">таблица OpenDocument</a> и
<a href="https://storage.googleapis.com/github-repositories-topic-modeling/topics.json">JSON</a> с извлеченными темами.</p>

<h2 id="выводы:3d1ad5dc492a464c9ab9cf6a149322ec">Выводы</h2>

<p>Как уже было написано выше, 200 тем это слишком мало, много тем оказались двойственными, тройственными или слабо выраженными. При выполнении анализа &ldquo;на чистовую&rdquo; нам стоит выставить 500 или 1000, однако придется забыть о ручной маркировке тем. Трудно разобраться в бесконечном количестве тем PHP, если ты не в теме :). Мне определенно пригодилось многолетнее чтение статей на Хабре, и все равно я чувствовал себя некомфортно. Но все равно получилось интересно. Выдающееся достижение ARTM на мой взгляд - извлечение тем про людей, природу, науку и даже шаблоны проектирования всего лишь из имен в исходниках.</p>

<p>В планах добавить к модели файлы readme и возможно другие текстовые источники. Наверное, они усилят группу <em>Понятия</em>.</p>

<h2 id="p-s:3d1ad5dc492a464c9ab9cf6a149322ec">P.S.</h2>

<p>Майнинг исходного кода в трактовке классического машинного обучения (а не тяп ляп собрали метрик из AST и в продакшен) - штука новая, пока что не очень популярная, научных статей практически нет. В перспективе, мы хотим и примерно представляем как заменить часть программистов глубокой нейронной сетью, которая будет транслировать описание бизнес задач на естественном языке в код. Звучит фантастически, но технологии на самом деле созрели и если получится, произойдет революция покруче индустриализации. Людей катастрофически не хватает! Мы хайрим!</p>

<p>Основная трудность в этом деле - получить доступ к данным. GitHub API лимитирует число запросов у зарегистрированных пользователей числом 5000 в час, что конечно же мало, если хотим заполучить 18кк. Есть проект <a href="http://ghtorrent.org/">GHTorrent</a>, но это лишь бледная тень тех данных, что собрали мы. Пришлось сделать особый конвейер на Go, который использует <a href="https://github.com/src-d/go-git">Go-Git</a> для сверхэффективного клонирования. Насколько нам известно, полная реплика GitHub есть у трех компаний: GitHub, <a href="https://sourcegraph.com/">SourceGraph</a> и source{d}.</p>

	        </section>
	    </article>
	</main>

    <footer class="site-footer">
	<div class="inner">
		<section class="footer-social">
			
		    <a href="//twitter.com/srcd_" target="_blank" title="Twitter"><i class="fa fa-2x fa-fw fa-twitter"></i> <span class="hidden">Twitter</span></a>&nbsp;
		    
		    
		    <a href="//github.com/src-d" target="_blank" title="GitHub"><i class="fa fa-2x fa-fw fa-github"></i> <span class="hidden">GitHub</span></a>&nbsp;
		    
		    
		</section>

		<section class="copyright">&copy; 2016 <a href="/">source{d}</a>. Crafted with love.</section>
	</div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script src="/js/smooth-scroll.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/highlight.min.js"></script>

<script>
    smoothScroll.init({
        speed: 800,
        easing: 'easeInOutCubic',
        updateURL: false,
        offset: 125,
    });
</script>
<script>hljs.initHighlightingOnLoad();</script>


<!-- Google Analytics -->
<script>
  var _gaq=[['_setAccount','UA-69608223-2'],['_trackPageview']];
  (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
  g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
  s.parentNode.insertBefore(g,s)}(document,'script'));
</script>



<script>
   (function(h,o,t,j,a,r){
       h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
       h._hjSettings={hjid:184958,hjsv:5};
       a=o.getElementsByTagName('head')[0];
       r=o.createElement('script');r.async=1;
       r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
       a.appendChild(r);
   })(window,document,'//static.hotjar.com/c/hotjar-','.js?sv=');
</script>

<script type="text/javascript">
    adroll_adv_id = "IYZYMSZIZZGURCHNYRRKTA";
    adroll_pix_id = "MXWTIGH3ZNCXDAD4UGQIW5";
     
     
    (function () {
        var _onload = function(){
            if (document.readyState && !/loaded|complete/.test(document.readyState)){setTimeout(_onload, 10);return}
            if (!window.__adroll_loaded){__adroll_loaded=true;setTimeout(_onload, 50);return}
            var scr = document.createElement("script");
            var host = (("https:" == document.location.protocol) ? "https://s.adroll.com" : "http://a.adroll.com");
            scr.setAttribute('async', 'true');
            scr.type = "text/javascript";
            scr.src = host + "/j/roundtrip.js";
            ((document.getElementsByTagName('head') || [null])[0] ||
                document.getElementsByTagName('script')[0].parentNode).appendChild(scr);
        };
        if (window.addEventListener) {window.addEventListener('load', _onload, false);}
        else {window.attachEvent('onload', _onload)}
    }());
</script>


</body>
</html>
