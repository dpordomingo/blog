<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Upcomings on source{d} blog</title>
    <link>/upcoming/</link>
    <description>Recent content in Upcomings on source{d} blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <atom:link href="/upcoming/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title></title>
      <link>/upcoming/github_topic_modeling_habrahabr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/upcoming/github_topic_modeling_habrahabr/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://blog.sourced.tech/post/github_topic_modeling/wordcloud.png&#34; alt=&#34;word cloud&#34; /&gt;
&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5&#34;&gt;Тематическое моделирование&lt;/a&gt; - подраздел машинного обучения, посвященный извлечению абстрактных &amp;ldquo;тем&amp;rdquo; из набора &amp;ldquo;документов&amp;rdquo;. Каждый &amp;ldquo;документ&amp;rdquo; представлен &lt;a href=&#34;https://en.wikipedia.org/wiki/Bag-of-words_model&#34;&gt;мешком слов&lt;/a&gt;, т.е. множеством слов вместе с их частотами. Введение в тематическое моделирование прекрасно описано проф. &lt;a href=&#34;http://www.machinelearning.ru/wiki/index.php?title=%D0%A3%D1%87%D0%B0%D1%81%D1%82%D0%BD%D0%B8%D0%BA:Vokov&#34;&gt;К. В. Воронцовым&lt;/a&gt; в лекциях ШАД [&lt;a href=&#34;http://www.machinelearning.ru/wiki/images/e/e6/Voron-ML-TopicModeling-slides.pdf&#34;&gt;PDF&lt;/a&gt;]. Самая известная модель ТМ - это, конечно, &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9B%D0%B0%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D0%B5_%D0%94%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5&#34;&gt;Латентное размещение Дирихле&lt;/a&gt; (LDA). Константину Вячеславовичу удалось обобщить все возможные тематические модели на основе мешка слов в виде &lt;a href=&#34;http://link.springer.com/article/10.1007/s10994-014-5476-6&#34;&gt;аддитивной регуляризации&lt;/a&gt; (ARTM). В частности, LDA тоже входит в множество моделей ARTM. Идеи ARTM воплощены в проекте &lt;a href=&#34;https://github.com/bigartm/bigartm&#34;&gt;BigARTM&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Обычно тематическое моделирование применяют к текстовым документам. Мы в source{d}
(стартап в Испании) перевариваем биг дату, полученную из GitHub репозиториев (
и скоро примемся за каждый публично доступный репозиторий в мире). Естественным
образом возникла идея интерпретировать каждый репозиторий как мешок слов и натравить BigARTM. В этой статье пойдет речь о том как мы выполнили по сути первое в мире тематическое исследование крупнейшего хранилища open source проектов, что из этого получилось и как это повторить. &lt;strong&gt;docker inside!&lt;/strong&gt;
&lt;habracut/&gt;&lt;/p&gt;

&lt;p&gt;TL;DR:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run srcd/github_topics apache/spark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(заменить &lt;code&gt;apache/spark&lt;/code&gt; на любой GitHub реп по желанию).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/github-repositories-topic-modeling/topics.ods&#34;&gt;Таблица OpenDocument с извлеченными темами.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/github-repositories-topic-modeling/topics.json&#34;&gt;JSON с извлеченными темами.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/github-repositories-topic-modeling/repo_topic_modeling.pickle.gz&#34;&gt;Обученная модель&lt;/a&gt; - 40МБ, gzipped pickle для Python 3.4+, Pandas 1.18+.&lt;/p&gt;

&lt;p&gt;[Dataset на data.world]() - скоро загрузим.&lt;/p&gt;

&lt;h2 id=&#34;теория:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Теория&lt;/h2&gt;

&lt;p&gt;Задана тематическая вероятностная модель на множестве документов
$$\inline D$$ которая описывает частоту появления слова $$\inline w$$ в документе $$\inline d$$ с темами $$\inline t$$:&lt;/p&gt;

&lt;p&gt;$$
p(w|d) = \sum_{t\in T} p(w|t) p(t|d)
$$&lt;/p&gt;

&lt;p&gt;где $$\inline p(w|t)$$ - вероятность отношения слова $$\inline w$$ к теме $$\inline t$$,
$$\inline p(t|d)$$ - вероятность отношения темы $$\inline t$$ к документу $$\inline d$$,
т.о. формула выше это просто выражение &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A4%D0%BE%D1%80%D0%BC%D1%83%D0%BB%D0%B0_%D0%BF%D0%BE%D0%BB%D0%BD%D0%BE%D0%B9_%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8&#34;&gt;полной вероятности&lt;/a&gt;,
при условии истинности гипотезы независимости случайных величин: $$\inline p(w|d,t) = p(w|t)$$.
Слова берутся из словаря $$\inline W$$, темы принадлежат множеству $$\inline T$$ которое представляет собой просто серию индексов $$\inline [1, 2, \dots n_t]$$.&lt;/p&gt;

&lt;p&gt;Нам нужно восстановить $$\inline p(w|t)$$ и $$\inline p(t|d)$$ из
заданного набора документов $$\inline \left{d\in D: d = \left{w&lt;em&gt;1 \dots w&lt;/em&gt;{n&lt;em&gt;d}\right}\right}$$.
Обычно считают что $$\inline \hat{p}(w|d) = \frac{n&lt;/em&gt;{dw}}{n&lt;em&gt;d}$$, где $$\inline n&lt;/em&gt;{dw}$$ -
количество вхождений $$\inline w$$ в документ $$\inline d$$,
однако это подразумевает что все слова одинаково важные что не всегда справедливо.
Под &amp;ldquo;важностью&amp;rdquo; здесь имеется в виду мера, негативно скоррелированная с общей частотой появления слова в документах.
Обозначим восстанавливаемые вероятности $$\inline \hat{p}(w|t) = \phi&lt;em&gt;{wt}$$ и $$\inline \hat{p}(t|d) = \theta&lt;/em&gt;{td}$$.
Т.о. наша задача сводится к стохастическому матричному разложению, которая некорректно поставлена:&lt;/p&gt;

&lt;p&gt;$$
\frac{n_{dw}}{n_d} ≈ \Phi \cdot \Theta = (\Phi S)(S^{-1}\Theta) = \Phi&amp;rsquo; \cdot \Theta&amp;rsquo;
$$&lt;/p&gt;

&lt;p&gt;В задачах машинного обучения обычно применяют &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0&#34;&gt;регуляризацию&lt;/a&gt;) как способ улучшить характеристики модели на неизвестных данных (как следствие, уменьшается &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&#34;&gt;переобучение&lt;/a&gt;, сложность и т.д.); в нашем случае, она просто &lt;strong&gt;необходима&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Задачи наподобие описанной выше решаются с помощью
&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%BE%D0%BF%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D1%8F&#34;&gt;метода максимального правдоподобия&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;$$
\sum&lt;em&gt;{d\in D}\sum&lt;/em&gt;{w\in d}n&lt;em&gt;{dw}\ln \sum&lt;/em&gt;{t}\phi&lt;em&gt;{wt} \theta&lt;/em&gt;{td} \to \max_{\Phi,\Theta}
$$&lt;/p&gt;

&lt;p&gt;при условиях&lt;/p&gt;

&lt;p&gt;$$
\phi&lt;em&gt;{wt} &amp;gt; 0; \sum&lt;/em&gt;{w\in W}\phi&lt;em&gt;{wt} = 1;
\theta&lt;/em&gt;{td} &amp;gt; 0; \sum&lt;em&gt;{t\in T}\theta&lt;/em&gt;{td} = 1.
$$&lt;/p&gt;

&lt;p&gt;Суть ARTM в том чтобы естественным образом добавить регуляризацию в виде дополнительных слагаемых:&lt;/p&gt;

&lt;p&gt;$$
\sum&lt;em&gt;{d\in D}\sum&lt;/em&gt;{w\in d}n&lt;em&gt;{dw}\ln \sum&lt;/em&gt;{t}\phi&lt;em&gt;{wt} \theta&lt;/em&gt;{td} + R(\Phi,\Theta) \to \max_{\Phi,\Theta}
$$&lt;/p&gt;

&lt;p&gt;Поскольку это простое сложение, мы можем комбинировать различные регуляризаторы в одной оптимизации,
например, проредить матрицу и увеличить независимость тем. LDA формулируется в терминах ARTM так:&lt;/p&gt;

&lt;p&gt;$$
R(\Phi,\Theta)&lt;em&gt;{Dirichlet} = \sum&lt;/em&gt;{t,w} (\beta&lt;em&gt;w - 1)\ln \phi&lt;/em&gt;{wt} + \sum_{d,t} (\alpha&lt;em&gt;t - 1)\ln \theta&lt;/em&gt;{t,d}
$$&lt;/p&gt;

&lt;p&gt;Переменные $$\inline \Phi$$ и $$\inline \Theta$$ могут быть эффективно вычислены с помощью итеративного
&lt;a href=&#34;https://ru.wikipedia.org/wiki/EM-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC&#34;&gt;EM-алгоритма&lt;/a&gt;.
Десятки готовых ARTM регуляризаторов готовы к бою в составе &lt;a href=&#34;https://github.com/bigartm/bigartm&#34;&gt;BigARTM&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;На этом вынужденный рерайт лекции ШАДа заканчивается и начинается&lt;/p&gt;

&lt;h2 id=&#34;практика:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Практика&lt;/h2&gt;

&lt;p&gt;Для анализа на октябрь 2016 года были доступны около 18 миллионов репозиториев на GitHub. Их на самом деле гораздо больше, просто мы отбросили форки и &amp;ldquo;хард форки&amp;rdquo; (форк не отмечен GitHub-ом).  Положим каждый репозиторий это $$d$$, а каждое имя в исходниках это $$w$$.
Анализ исходников делался теми же инструментами, что и при глубоком обучении исходному коду в ранних экспериментах
(см. наши презентации с последних конференций &lt;a href=&#34;https://www.re-work.co/&#34;&gt;RE·WORK&lt;/a&gt;:
&lt;a href=&#34;https://goo.gl/4zq8g9&#34;&gt;Берлин&lt;/a&gt; и &lt;a href=&#34;https://goo.gl/wRQCLS&#34;&gt;Лондон&lt;/a&gt;): первичная классификация
&lt;a href=&#34;https://github.com/github/linguist&#34;&gt;github/linguist&lt;/a&gt;-ом и парсинг на основе &lt;a href=&#34;http://pygments.org/&#34;&gt;Pygments&lt;/a&gt;.
Текстовые файлы общего назначения отбрасывались, например README.md.&lt;/p&gt;

&lt;p&gt;Имена из исходников следует извлекать не &amp;ldquo;в лоб&amp;rdquo;, например, &lt;code&gt;class FooBarBaz&lt;/code&gt; добавляет 3 слова в мешок: &lt;code&gt;foo&lt;/code&gt;, &lt;code&gt;bar&lt;/code&gt; и &lt;code&gt;baz&lt;/code&gt;, а
&lt;code&gt;int wdSize&lt;/code&gt; добавляет два: &lt;code&gt;wdsize&lt;/code&gt; и &lt;code&gt;size&lt;/code&gt;. Кроме того, имена стеммировались &lt;a href=&#34;http://snowballstem.org/&#34;&gt;Snowball&lt;/a&gt;-ом из &lt;a href=&#34;http://www.nltk.org/&#34;&gt;NLTK&lt;/a&gt;, хотя специально мы не исследовали пользу от этого. Последний этап предобработки заключался в вычислении логарифмической версии
&lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;TF-IDF&lt;/a&gt; взвешивания
(снова, специально не исследовали, просто копировали
&lt;a href=&#34;https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering&#34;&gt;решения&lt;/a&gt; из обычного НЛП) и фильтрации слишком редких и общеупотребительных имен, в нашем случае, границы были 50 и 100000 соответственно.&lt;/p&gt;

&lt;p&gt;После того как ARTM выдал результат, нужно было вручную дать имена темам, основываясь на
ключевых словах и репозиториях-представителях. Число тем было выставлено в 200, и как потом оказалось,
нужно было поставить больше, т.к. тем на Гитхабе очень много. Нудная работа отняла целую неделю.&lt;/p&gt;

&lt;p&gt;Предобработка была выполнена на &lt;a href=&#34;https://cloud.google.com/dataproc/&#34;&gt;Dataproc&lt;/a&gt; aka Spark в облаке Google, а основные действия производились локально на мощном компьютере. Результирующая разреженная матрица имела размер около 20 GB, и ее пришлось преобразовать в текстовый формат Vowpal Wabbit, чтобы ее смог переварить BigARTM CLI. Данные были перемолоты довольно быстро, за пару часов:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bigartm -c dataset_vowpal_wabbit.txt -t 200 -p 10 --threads 10 --write-model-readable bigartm.txt --regularizer &amp;quot;0.05 SparsePhi&amp;quot; &amp;quot;0.05 SparseTheta&amp;quot;
Parsing text collection... OK.  
Gathering dictionary from batches... OK.  
Initializing random model from dictionary... OK.  
Number of tokens in the model: 604989
================= Processing started.
Perplexity      = 586350
SparsityPhi     = 0.00214434
SparsityTheta   = 0.422496
================= Iteration 1 took 00:11:57.116
Perplexity      = 107901
SparsityPhi     = 0.00613982
SparsityTheta   = 0.552418
================= Iteration 2 took 00:12:03.001
Perplexity      = 60701.5
SparsityPhi     = 0.102947
SparsityTheta   = 0.768934
================= Iteration 3 took 00:11:55.172
Perplexity      = 20993.5
SparsityPhi     = 0.458439
SparsityTheta   = 0.902972
================= Iteration 4 took 00:11:56.804
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;-p&lt;/code&gt; задает число итераций. Уверенности в том, какие регуляризаторы использовать, не было, так что активирована только &amp;ldquo;sparsity&amp;rdquo;. Сказалось отсутствие подробной документации (разработчики пообещали это исправить). Важно отметить, что объем оперативной памяти, который потребовался для работы на пике был не больше 30 GB, что очень круто на фоне &lt;a href=&#34;https://radimrehurek.com/gensim/&#34;&gt;gensim&lt;/a&gt; и, прости господи,
&lt;a href=&#34;http://scikit-learn.org/stable/&#34;&gt;sklearn&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;темы:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Темы&lt;/h2&gt;

&lt;p&gt;В итоге, 200 тем можно поделить на следующие группы:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Понятия&lt;/strong&gt; - общие, гирокие и абстрактные.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Человеческие языки&lt;/strong&gt; - оказалось, что по коду можно примерно определить родной язык программиста, наверное, отчасти благодаря смещению от стемминга.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Языки программирования&lt;/strong&gt; - не то чтобы интересные, т.к. эту информацию мы и так знаем.
У ЯП обычно есть стандартная библиотека aka &amp;ldquo;батарейки&amp;rdquo; из классов и функций, которые импортируются/включаются в исходники, и соответствующие имена детектируются нашим тематическим моделированием.
Некоторые темы оказались у́же чем ЯП.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Общий IT&lt;/strong&gt; - попали бы в &lt;em&gt;Понятия&lt;/em&gt; если имели выразительный список ключевых слов. Репозитории часто ассоциируются с уникальным набором имен, например, говорим Рельсы, держим в уме ActiveObject и прочий Active. Частично перекликается с &lt;a href=&#34;https://habrahabr.ru/post/247363/&#34;&gt;Философия программирования 2 — Миф и язык&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сообщества&lt;/strong&gt; - посвященные конкретным, потенциально узким технологиям или продуктам.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Игры&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Бред&lt;/strong&gt; - 2 темам так и не удалось найти разумное объяснение.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;понятия:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Понятия&lt;/h3&gt;

&lt;p&gt;Пожалуй, самая интересная группа с кучей извлеченных фактов из повседневной жизни:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;В пицце есть сыр, а еще ее упоминают многие репозитории.&lt;/li&gt;
&lt;li&gt;Термины из математики, линейной алгебры, криптографии, машинного обучения, цифровой обработки сигналов, генной инженерии, физики элементарных частиц.&lt;/li&gt;
&lt;li&gt;Дни недели. Monday, Tuesday и т.д.&lt;/li&gt;
&lt;li&gt;Всевозможные факты и персонажи из RPG и других игр в жанре фэнтези.&lt;/li&gt;
&lt;li&gt;В IRC есть псевдонимы.&lt;/li&gt;
&lt;li&gt;Множество шаблонов проектирования (говорим за них спасибо Java и PHP).&lt;/li&gt;
&lt;li&gt;Цвета. Включая некоторые экзотические (говорим за них спасибо &lt;a href=&#34;https://habrahabr.ru/company/ua-hosting/blog/269013/&#34;&gt;CSS&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;В электронной почте есть CC, BCC, и ее посылают протоколом SMTP и получают POP/IMAP-ом.&lt;/li&gt;
&lt;li&gt;Как создать хороший datetime picker. Кажется, это весьма типичный проект на GitHub, хе-хе.&lt;/li&gt;
&lt;li&gt;Люди работают за деньги и тратят их на покупку домов и вождение (очевидно, из дома на работу и назад).&lt;/li&gt;
&lt;li&gt;Всевозможное &amp;ldquo;железо&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Исчерпывающий список терминов HTTP, SSL, Internet, Bluetooth и WiFi.&lt;/li&gt;
&lt;li&gt;Все что вы хотели бы узнать про управление памятью.&lt;/li&gt;
&lt;li&gt;Что гуглить есть хочется сделать свою прошивку на основе Android.&lt;/li&gt;
&lt;li&gt;Штрихкоды. Огромное число разных видов.&lt;/li&gt;
&lt;li&gt;Люди. Они делятся на мужчин и женщин, они живут и занимаются сексом.&lt;/li&gt;
&lt;li&gt;Отличный список текстовых редакторов.&lt;/li&gt;
&lt;li&gt;Погода. Много типичных слов.&lt;/li&gt;
&lt;li&gt;Открытые лицензии. Вообще говоря, они не должны были попасть в отдельную тему т.к. имена и тексты лицензий в теории не пересекаются. По опыта работы с Pygments, некоторые ЯП поддерживаются гораздо хуже чем другие и, судя по всему, некоторые были неправильно распарсены.&lt;/li&gt;
&lt;li&gt;Коммерция. У магазины предлагают скидки и продают товары покупателям.&lt;/li&gt;
&lt;li&gt;Биткоины и блокчейн.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;человеческие-языки:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Человеческие языки&lt;/h3&gt;

&lt;p&gt;В список тем вошли испанский, португальский, французский и китайский. Русский в отдельную тему не сформировался, что свидетельствует скорее о более высоком уровне наших программистов на GitHub, пишущих сразу на английском, чем о малом количестве русских репозиториев. В этом смысле, китайские репозитории убивают.&lt;/p&gt;

&lt;h3 id=&#34;языки-программирования:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Языки программирования&lt;/h3&gt;

&lt;p&gt;Интересная находка в ЯП - это тема &amp;ldquo;Non-native English PHP&amp;rdquo;, ассоциированная с проектами на PHP, не написанными людьми, знающими английский. Видимо, эти две группы программистов как-то принципиально по-разному пишут код. Кроме того, есть две темы, относящиеся к Java: JNI и байткод.&lt;/p&gt;

&lt;h3 id=&#34;общий-it:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Общий IT&lt;/h3&gt;

&lt;p&gt;Здесь не так интересно. Существует много репозиториев с ядрами ОС - большие, шумные и несмотря на наши старания они изгадили некоторые топики. Тем не менее, кое-что стоит упомянуть:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Много информации о дронах. Они работают на Linux.&lt;/li&gt;
&lt;li&gt;Есть много реализаций Ruby. Часто встречаются &amp;ldquo;экстремальные форки&amp;rdquo;, когда люди берут чужую кодовую базу и комиттят единым целым с потерей истории изменений.&lt;/li&gt;
&lt;li&gt;onmouseup, onmousedown и onmousemove - три гиганта, на которых стоит UI.&lt;/li&gt;
&lt;li&gt;Огромное число баззвордов и технологий из мира Javascript.&lt;/li&gt;
&lt;li&gt;Платформы для онлайн обучения. Особенно &lt;a href=&#34;https://moodle.org/&#34;&gt;Moodle&lt;/a&gt;. Много, много Moodle.&lt;/li&gt;
&lt;li&gt;Все когда-либо созданные открытые CMS.&lt;/li&gt;
&lt;li&gt;Тема &amp;ldquo;Coursera Machine Learning&amp;rdquo; предоставляет отличный список репозиториев с домашней работой на курсах Coursera, посвященных машинному обучению.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;сообщества:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Сообщества&lt;/h3&gt;

&lt;p&gt;Самая большая по численности тем группа, почти 100. Много репозиториев оказались частными облачными хранилищами конфигураций для текстовых редакторов, особенно Vim и Emacs. Т.к. у Vim всего одна тема, в то время как у Emacs две, надеюсь, это поставит окончательную точку в споре какой редактор лучше!&lt;/p&gt;

&lt;p&gt;Встретились сайты на всех известных веб движках, написанных на Python, Ruby, PHP, Java, Javascript и т.д. Сайты на PHP используют движки Wordpress, Joomla, Yii, VTiger, Drupal, Zend, Cake и Symphony почему-то обязательно с Doctrine (по теме на каждый).
Python: Django, Flask, Google AppEngine. Ruby: Rails и только Rails. &lt;a href=&#34;https://habrahabr.ru/post/301532/&#34;&gt;Реееельсы&lt;/a&gt;. Сайты на Java
сколлапсировали в одну смешанную тему. И конечно же нашлось место сайтам на Node.js.&lt;/p&gt;

&lt;p&gt;Оказалось, что много проектов используют &lt;a href=&#34;https://github.com/tesseract-ocr/tesseract&#34;&gt;Tesseract&lt;/a&gt; - открытый движок OCR. Кроме того, многие используют &lt;a href=&#34;https://github.com/BVLC/caffe&#34;&gt;Caffe&lt;/a&gt; (и ни разу не Tensorflow).&lt;/p&gt;

&lt;p&gt;Quake 3 / idTech 3 настолько популярен в геймдеве что заслужил отдельную тему. У Unity3D  их две, причем первая в основной массе это сборище студенческих проектов и домашних поделок.
Cocos2D также популярен и получил 2 темы. Наконец, нашлись 3 темы про OpenGL + WebGL.
Наверное, разница в способе работы с API и используемой обвязке (GLUT и т.п.).&lt;/p&gt;

&lt;p&gt;Неудивительно, что &lt;a href=&#34;https://github.com/chef/chef&#34;&gt;Chef&lt;/a&gt; - средство управления конфигурациями -
поделил тему с готовкой еды (recipes, kitchen и т.п).
Однако, WinAPI неожиданно оказался в одной теме с репозиториями про покемонов. Есть предположение, что стемминг сделал характерные имена WinAPI похожими на имена покемонов&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;игры:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Игры&lt;/h3&gt;

&lt;p&gt;Много тем связаны с &lt;a href=&#34;http://www.libsdl.org/&#34;&gt;SDL&lt;/a&gt;, а также с Minecraft и RPG.&lt;/p&gt;

&lt;h2 id=&#34;что-можно-скачать:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Что можно скачать&lt;/h2&gt;

&lt;p&gt;Мы приготовили образ Docker, чтобы любой желающий мог запустить нашу обученную модель
на произвольном репозитории с GitHub. Нужно просто выполнить&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run srcd/github_topics apache/spark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;и вы увидите топ 5. Внутри образа зашита сериализованная матрица тем и слов, она доступна отдельно: &lt;a href=&#34;https://storage.googleapis.com/github-repositories-topic-modeling/repo_topic_modeling.pickle.gz&#34;&gt;link&lt;/a&gt;.
Формат - &lt;a href=&#34;https://docs.python.org/3/library/pickle.html#pickle-protocols&#34;&gt;pickle 4-ой версии&lt;/a&gt;
с кортежем длины 2, первый элемент это Pandas 1.8+ &lt;a href=&#34;http://pandas.pydata.org/pandas-docs/stable/sparse.html?highlight=sparsedataframe&#34;&gt;SparseDataFrame&lt;/a&gt;,
и второй - список с IDF. Кроме того, есть
&lt;a href=&#34;https://storage.googleapis.com/github-repositories-topic-modeling/topics.ods&#34;&gt;таблица OpenDocument&lt;/a&gt; и
&lt;a href=&#34;https://storage.googleapis.com/github-repositories-topic-modeling/topics.json&#34;&gt;JSON&lt;/a&gt; с извлеченными темами.&lt;/p&gt;

&lt;h2 id=&#34;выводы:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;Выводы&lt;/h2&gt;

&lt;p&gt;Как уже было написано выше, 200 тем это слишком мало, много тем оказались двойственными, тройственными или слабо выраженными. При выполнении анализа &amp;ldquo;на чистовую&amp;rdquo; нам стоит выставить 500 или 1000, однако придется забыть о ручной маркировке тем. Трудно разобраться в бесконечном количестве тем PHP, если ты не в теме :). Мне определенно пригодилось многолетнее чтение статей на Хабре, и все равно я чувствовал себя некомфортно. Но все равно получилось интересно. Выдающееся достижение ARTM на мой взгляд - извлечение тем про людей, природу, науку и даже шаблоны проектирования всего лишь из имен в исходниках.&lt;/p&gt;

&lt;p&gt;В планах добавить к модели файлы readme и возможно другие текстовые источники. Наверное, они усилят группу &lt;em&gt;Понятия&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;p-s:3d1ad5dc492a464c9ab9cf6a149322ec&#34;&gt;P.S.&lt;/h2&gt;

&lt;p&gt;Майнинг исходного кода в трактовке классического машинного обучения (а не тяп ляп собрали метрик из AST и в продакшен) - штука новая, пока что не очень популярная, научных статей практически нет. В перспективе, мы хотим и примерно представляем как заменить часть программистов глубокой нейронной сетью, которая будет транслировать описание бизнес задач на естественном языке в код. Звучит фантастически, но технологии на самом деле созрели и если получится, произойдет революция покруче индустриализации. Людей катастрофически не хватает! Мы хайрим!&lt;/p&gt;

&lt;p&gt;Основная трудность в этом деле - получить доступ к данным. GitHub API лимитирует число запросов у зарегистрированных пользователей числом 5000 в час, что конечно же мало, если хотим заполучить 18кк. Есть проект &lt;a href=&#34;http://ghtorrent.org/&#34;&gt;GHTorrent&lt;/a&gt;, но это лишь бледная тень тех данных, что собрали мы. Пришлось сделать особый конвейер на Go, который использует &lt;a href=&#34;https://github.com/src-d/go-git&#34;&gt;Go-Git&lt;/a&gt; для сверхэффективного клонирования. Насколько нам известно, полная реплика GitHub есть у трех компаний: GitHub, &lt;a href=&#34;https://sourcegraph.com/&#34;&gt;SourceGraph&lt;/a&gt; и source{d}.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>